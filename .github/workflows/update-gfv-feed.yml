name: Update feed.json (GFV mentions)

on:
  workflow_dispatch:
  schedule:
    - cron: "*/30 * * * *"

concurrency:
  group: notion-news-feed-gfv
  cancel-in-progress: true

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Fetch Google News RSS and write feed.json
        run: |
          python3 - << 'PY'
          import json, re, urllib.parse, urllib.request
          import xml.etree.ElementTree as ET
          from datetime import datetime, timezone, timedelta
          from email.utils import parsedate_to_datetime

          MAX_ITEMS = 200
          MAX_AGE_DAYS = 180

          HL = "en-GB"
          GL = "GB"
          CEID = "GB:en"

          QUERIES = [
              '"Green Flag Ventures"',
              '"Justin Zeefe"',
              '"Deborah Fairlamb"',
              'GFV -("gross floor value") -("good faith violation") -("gross floor")',
              '("Green Flag Ventures" OR "Justin Zeefe" OR "Deborah Fairlamb" OR GFV) (venture OR VC OR fund OR investment OR portfolio OR backs OR backed OR led OR seed)',
          ]

          cutoff = datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)
          cutoff_date = cutoff.date().isoformat()

          def clean_html(s: str) -> str:
              if not s:
                  return ""
              s = re.sub(r"<br\s*/?>", " ", s, flags=re.I)
              s = re.sub(r"<[^>]+>", "", s)
              s = re.sub(r"\s+", " ", s).strip()
              return s

          def parse_pubdate(s: str):
              try:
                  dt = parsedate_to_datetime(s)
                  if dt.tzinfo is None:
                      dt = dt.replace(tzinfo=timezone.utc)
                  return dt.astimezone(timezone.utc)
              except Exception:
                  return None

          items = []
          for q in QUERIES:
              q2 = f"{q} after:{cutoff_date}"
              url = "https://news.google.com/rss/search?" + urllib.parse.urlencode({
                  "q": q2,
                  "hl": HL,
                  "gl": GL,
                  "ceid": CEID,
              })

              try:
                  with urllib.request.urlopen(url, timeout=30) as resp:
                      xml = resp.read()
              except Exception as e:
                  print("Fetch failed:", q, e)
                  continue

              root = ET.fromstring(xml)
              channel = root.find("channel")
              if channel is None:
                  continue

              for it in channel.findall("item"):
                  title = (it.findtext("title") or "").strip()
                  link = (it.findtext("link") or "").strip()
                  pub = (it.findtext("pubDate") or "").strip()
                  src_el = it.find("source")
                  source = (src_el.text or "").strip() if src_el is not None and src_el.text else ""
                  desc = clean_html((it.findtext("description") or "").strip())

                  dt = parse_pubdate(pub)
                  if not dt or dt < cutoff:
                      continue

                  items.append({
                      "headline": title,
                      "source": source,
                      "link": link,
                      "pubDate": dt.strftime("%a, %d %b %Y %H:%M:%S GMT"),
                      "pubDateISO": dt.isoformat(),
                      "snippet": desc,
                      "query": q,
                  })

          seen = set()
          out = []
          for x in sorted(items, key=lambda z: z["pubDateISO"], reverse=True):
              key = x["link"] or (x["headline"] + "|" + x["pubDate"])
              if key in seen:
                  continue
              seen.add(key)
              out.append(x)
              if len(out) >= MAX_ITEMS:
                  break

          payload = {"generatedAt": datetime.now(timezone.utc).isoformat(), "items": out}
          with open("feed.json", "w", encoding="utf-8") as f:
              json.dump(payload, f, ensure_ascii=False, indent=2)

          print("Wrote", len(out), "items")
          PY

      - name: Commit feed.json
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Update feed.json"
          file_pattern: "feed.json"
          push_options: "--force"

      - name: Install Python deps
        run: |
          python3 -m pip install --upgrade pip
          pip3 install requests fpdf2 google-api-python-client google-auth trafilatura

      - name: Sync feed.json to Notion database (with PDF archive)
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
          GOOGLE_SERVICE_ACCOUNT_JSON: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_JSON }}
          GOOGLE_DRIVE_FOLDER_ID: ${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}
        run: |
          python3 - << 'PY'
          import os, json, re, time, textwrap, tempfile
          import urllib.request, urllib.error
          import requests
          import trafilatura
          from fpdf import FPDF
          from fpdf.enums import XPos, YPos
          from google.oauth2 import service_account
          from googleapiclient.discovery import build
          from googleapiclient.http import MediaFileUpload

          # ── helpers ──────────────────────────────────────────────────────────

          def clean_secret(s: str) -> str:
              s = (s or "").strip().strip('"').strip("'")
              return s.replace("\n", "").replace("\r", "").replace("\t", "").strip()

          def resolve_google_news_url(gn_url: str) -> str:
              """Follow Google News redirect to get the real article URL."""
              try:
                  req = urllib.request.Request(
                      gn_url,
                      headers={"User-Agent": "Mozilla/5.0 (compatible; GFVBot/1.0)"}
                  )
                  with urllib.request.urlopen(req, timeout=15) as resp:
                      return resp.url
              except urllib.error.HTTPError as e:
                  return e.url if e.url else gn_url
              except Exception:
                  return gn_url

          def fetch_article_text(url: str) -> str:
              """Fetch and extract full article text using trafilatura."""
              try:
                  downloaded = trafilatura.fetch_url(url)
                  if not downloaded:
                      return ""
                  text = trafilatura.extract(
                      downloaded,
                      include_comments=False,
                      include_tables=False,
                      no_fallback=False,
                  )
                  return (text or "").strip()
              except Exception as e:
                  print(f"  trafilatura failed: {e}")
                  return ""

          def build_drive_service(sa_json: str):
              info = json.loads(sa_json)
              creds = service_account.Credentials.from_service_account_info(
                  info,
                  scopes=["https://www.googleapis.com/auth/drive.file"],
              )
              return build("drive", "v3", credentials=creds, cache_discovery=False)

          def add_text_block(pdf, text, style="", size=10,
                             color=(50, 50, 50), width=88, line_height=6):
              """Helper to add wrapped text block."""
              pdf.set_font("Helvetica", style, size)
              pdf.set_text_color(*color)
              for para in text.split("\n"):
                  para = para.strip()
                  if not para:
                      pdf.ln(3)
                      continue
                  for line in textwrap.wrap(para, width=width) or [""]:
                      pdf.cell(0, line_height, line,
                               new_x=XPos.LMARGIN, new_y=YPos.NEXT)

          def make_pdf(item: dict, real_url: str, article_text: str) -> str:
              """Generate a clean PDF with full article content."""
              pdf = FPDF()
              pdf.set_auto_page_break(auto=True, margin=15)
              pdf.add_page()

              # Header bar
              pdf.set_fill_color(30, 30, 30)
              pdf.rect(0, 0, 210, 18, "F")
              pdf.set_font("Helvetica", "B", 11)
              pdf.set_text_color(255, 255, 255)
              pdf.set_xy(8, 4)
              pdf.cell(0, 10, "GFV News Archive",
                       new_x=XPos.LMARGIN, new_y=YPos.NEXT)

              pdf.set_text_color(30, 30, 30)
              pdf.ln(6)

              # Headline
              headline = (item.get("headline") or "Untitled")
              add_text_block(pdf, headline, style="B", size=15,
                             color=(20, 20, 20), width=72, line_height=8)
              pdf.ln(3)

              # Meta
              source = item.get("source") or ""
              pub = item.get("pubDate") or ""
              query = item.get("query") or ""
              pdf.set_font("Helvetica", "", 9)
              pdf.set_text_color(100, 100, 100)
              pdf.cell(0, 6, f"Source: {source}   |   Published: {pub}",
                       new_x=XPos.LMARGIN, new_y=YPos.NEXT)
              pdf.cell(0, 6, f"Query match: {query}",
                       new_x=XPos.LMARGIN, new_y=YPos.NEXT)
              pdf.ln(4)

              # Divider
              pdf.set_draw_color(200, 200, 200)
              pdf.line(10, pdf.get_y(), 200, pdf.get_y())
              pdf.ln(5)

              # Real URL
              pdf.set_font("Helvetica", "B", 9)
              pdf.set_text_color(30, 30, 30)
              pdf.cell(0, 6, "Article URL:",
                       new_x=XPos.LMARGIN, new_y=YPos.NEXT)
              pdf.set_font("Helvetica", "", 9)
              pdf.set_text_color(0, 80, 180)
              for chunk in textwrap.wrap(real_url, width=95):
                  pdf.cell(0, 5, chunk, new_x=XPos.LMARGIN, new_y=YPos.NEXT)
              pdf.ln(5)

              # Article body
              pdf.set_draw_color(200, 200, 200)
              pdf.line(10, pdf.get_y(), 200, pdf.get_y())
              pdf.ln(5)

              if article_text:
                  pdf.set_font("Helvetica", "B", 10)
                  pdf.set_text_color(30, 30, 30)
                  pdf.cell(0, 7, "Article content:",
                           new_x=XPos.LMARGIN, new_y=YPos.NEXT)
                  pdf.ln(2)
                  add_text_block(pdf, article_text, size=10,
                                 color=(40, 40, 40), width=90, line_height=6)
              else:
                  snippet = (item.get("snippet") or "").strip()
                  if snippet:
                      pdf.set_font("Helvetica", "B", 10)
                      pdf.set_text_color(30, 30, 30)
                      pdf.cell(0, 7, "Article snippet (full text unavailable):",
                               new_x=XPos.LMARGIN, new_y=YPos.NEXT)
                      pdf.ln(2)
                      add_text_block(pdf, snippet, size=10,
                                     color=(80, 80, 80), width=90, line_height=6)
                  else:
                      pdf.set_font("Helvetica", "I", 9)
                      pdf.set_text_color(130, 130, 130)
                      pdf.cell(0, 6, "Full text unavailable. Visit the article URL above.",
                               new_x=XPos.LMARGIN, new_y=YPos.NEXT)

              # Footer
              pdf.set_y(-18)
              pdf.set_font("Helvetica", "I", 8)
              pdf.set_text_color(160, 160, 160)
              pdf.cell(0, 6,
                       f"Generated by GFV News Monitor  |  {time.strftime('%Y-%m-%d %H:%M UTC', time.gmtime())}",
                       new_x=XPos.LMARGIN, new_y=YPos.NEXT)

              tmp = tempfile.NamedTemporaryFile(suffix=".pdf", delete=False)
              pdf.output(tmp.name)
              return tmp.name

          def upload_to_drive(service, pdf_path: str, filename: str, folder_id: str) -> str:
              """Upload PDF and return a shareable https link."""
              meta = {"name": filename, "parents": [folder_id]}
              media = MediaFileUpload(pdf_path, mimetype="application/pdf", resumable=False)
              f = service.files().create(
                  body=meta,
                  media_body=media,
                  fields="id",
                  supportsAllDrives=True,
              ).execute()
              file_id = f["id"]
              service.permissions().create(
                  fileId=file_id,
                  body={"type": "anyone", "role": "reader"},
                  supportsAllDrives=True,
              ).execute()
              return f"https://drive.google.com/file/d/{file_id}/view"

          # ── main ─────────────────────────────────────────────────────────────

          token        = clean_secret(os.environ.get("NOTION_TOKEN"))
          db_raw       = clean_secret(os.environ.get("NOTION_DATABASE_ID")).replace("-", "")
          sa_json      = os.environ.get("GOOGLE_SERVICE_ACCOUNT_JSON", "").strip()
          drive_folder = clean_secret(os.environ.get("GOOGLE_DRIVE_FOLDER_ID"))

          if not token:
              raise RuntimeError("Missing GitHub secret: NOTION_TOKEN")
          if not re.fullmatch(r"[0-9a-fA-F]{32}", db_raw):
              raise RuntimeError(f"NOTION_DATABASE_ID invalid. Got length={len(db_raw)}")
          if not sa_json:
              raise RuntimeError("Missing GitHub secret: GOOGLE_SERVICE_ACCOUNT_JSON")
          if not drive_folder:
              raise RuntimeError("Missing GitHub secret: GOOGLE_DRIVE_FOLDER_ID")

          db_id = f"{db_raw[0:8]}-{db_raw[8:12]}-{db_raw[12:16]}-{db_raw[16:20]}-{db_raw[20:32]}"

          headers = {
              "Authorization": f"Bearer {token}",
              "Notion-Version": "2022-06-28",
              "Content-Type": "application/json",
          }

          # Preflight
          pre = requests.get(f"https://api.notion.com/v1/databases/{db_id}", headers=headers, timeout=30)
          if pre.status_code != 200:
              raise RuntimeError(f"Notion preflight failed: {pre.status_code}\n{pre.text}")

          # Build Drive service
          drive_svc = build_drive_service(sa_json)

          # Load feed
          with open("feed.json", "r", encoding="utf-8") as f:
              feed = json.load(f)
          items = feed.get("items", [])

          # Pull existing URLs from Notion
          existing_urls = set()
          start_cursor = None
          for _ in range(50):
              payload = {"page_size": 100}
              if start_cursor:
                  payload["start_cursor"] = start_cursor
              r = requests.post(
                  f"https://api.notion.com/v1/databases/{db_id}/query",
                  headers=headers, json=payload, timeout=30
              )
              if r.status_code != 200:
                  raise RuntimeError(f"Notion query failed: {r.status_code}\n{r.text}")
              data = r.json()
              for page in data.get("results", []):
                  u = (page.get("properties", {}).get("URL", {}) or {}).get("url")
                  if u:
                      existing_urls.add(u)
              if not data.get("has_more"):
                  break
              start_cursor = data.get("next_cursor")

          created = 0
          for it in items:
              url = it.get("link")
              if not url or url in existing_urls:
                  continue

              title   = (it.get("headline") or "Untitled")[:180]
              source  = (it.get("source") or "")[:200]
              query   = (it.get("query") or "")[:200]
              iso     = it.get("pubDateISO")

              # 1. Resolve real URL
              print(f"  Resolving URL for: {title[:60]}...")
              real_url = resolve_google_news_url(url)
              print(f"  Real URL: {real_url[:80]}")

              # 2. Fetch full article text
              article_text = fetch_article_text(real_url)
              if article_text:
                  print(f"  Got {len(article_text)} chars of article text")
              else:
                  print(f"  No article text (paywall or JS-heavy site, using snippet)")

              # 3. Generate PDF
              pdf_path = make_pdf(it, real_url, article_text)

              # 4. Upload to Drive
              safe_title = re.sub(r'[\\/*?:"<>|]', "", title)[:80]
              date_prefix = (iso or "")[:10]
              filename = f"{date_prefix} - {safe_title}.pdf"
              try:
                  drive_url = upload_to_drive(drive_svc, pdf_path, filename, drive_folder)
              except Exception as e:
                  print(f"  Drive upload failed: {e}")
                  drive_url = None
              finally:
                  os.unlink(pdf_path)

              # 5. Create Notion row
              props = {
                  "Name":   {"title": [{"text": {"content": title}}]},
                  "URL":    {"url": url},
                  "Source": {"rich_text": [{"text": {"content": source}}]} if source else {"rich_text": []},
                  "Query":  {"rich_text": [{"text": {"content": query}}]} if query else {"rich_text": []},
              }
              if isinstance(iso, str) and "T" in iso:
                  props["DATE"] = {"date": {"start": iso}}
              if drive_url:
                  props["PDF"] = {"url": drive_url}

              c = requests.post(
                  "https://api.notion.com/v1/pages",
                  headers=headers,
                  json={"parent": {"database_id": db_id}, "properties": props},
                  timeout=30,
              )
              if c.status_code != 200:
                  raise RuntimeError(f"Notion create failed: {c.status_code}\n{c.text}")

              existing_urls.add(url)
              created += 1
              print(f"  ✓ Created: {title[:60]}{'...' if len(title)>60 else ''}")
              if drive_url:
                  print(f"    PDF: {drive_url}")
              time.sleep(0.35)

          print(f"\nNotion sync complete: created={created}, existing={len(existing_urls)}")
          PY
