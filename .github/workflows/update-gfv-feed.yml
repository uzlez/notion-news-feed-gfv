name: Update feed.json (GFV mentions)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"

concurrency:
  group: notion-news-feed-gfv
  cancel-in-progress: true

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Fetch news and write feed.json
        env:
          NEWS_API_KEY: ${{ secrets.NEWS_API_KEY }}
        run: |
          python3 - << 'PY'
          import json, re, os, urllib.parse, urllib.request
          from datetime import datetime, timezone, timedelta

          MAX_ITEMS = 200
          MAX_AGE_DAYS = 30  # NewsAPI free tier only goes back 30 days

          api_key = os.environ.get("NEWS_API_KEY", "").strip()
          if not api_key:
              raise RuntimeError("Missing NEWS_API_KEY")

          QUERIES = [
              "Green Flag Ventures",
              "Justin Zeefe",
              "Deborah Fairlamb",
          ]

          cutoff = datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)

          def clean_html(s):
              if not s: return ""
              s = re.sub(r"<[^>]+>", "", s)
              s = re.sub(r"&nbsp;", " ", s)
              s = re.sub(r"&amp;", "&", s)
              s = re.sub(r"&lt;", "<", s)
              s = re.sub(r"&gt;", ">", s)
              s = re.sub(r"&quot;", '"', s)
              s = re.sub(r"&#?\w+;", "", s)
              return re.sub(r"\s+", " ", s).strip()

          items = []
          for q in QUERIES:
              url = "https://newsapi.org/v2/everything?" + urllib.parse.urlencode({
                  "q": q,
                  "language": "en",
                  "sortBy": "publishedAt",
                  "pageSize": 100,
                  "from": cutoff.strftime("%Y-%m-%d"),
                  "apiKey": api_key,
              })
              try:
                  with urllib.request.urlopen(url, timeout=30) as resp:
                      data = json.loads(resp.read())
              except Exception as e:
                  print(f"NewsAPI fetch failed for '{q}': {e}")
                  continue

              if data.get("status") != "ok":
                  print(f"NewsAPI error for '{q}': {data.get('message')}")
                  continue

              articles = data.get("articles", [])
              print(f"  '{q}': {len(articles)} articles")

              for art in articles:
                  title   = clean_html(art.get("title") or "")
                  link    = art.get("url") or ""
                  pub     = art.get("publishedAt") or ""
                  source  = (art.get("source") or {}).get("name") or ""
                  desc    = clean_html(art.get("description") or "")
                  content = clean_html(art.get("content") or "")

                  if not link or not title:
                      continue
                  if "[Removed]" in title:
                      continue

                  try:
                      dt = datetime.fromisoformat(pub.replace("Z", "+00:00"))
                  except Exception:
                      continue
                  if dt < cutoff:
                      continue

                  items.append({
                      "headline": title,
                      "source": source,
                      "link": link,
                      "pubDate": dt.strftime("%a, %d %b %Y %H:%M:%S GMT"),
                      "pubDateISO": dt.isoformat(),
                      "snippet": desc,
                      "content": content,
                      "query": q,
                  })

          seen = set()
          out = []
          for x in sorted(items, key=lambda z: z["pubDateISO"], reverse=True):
              key = x["link"]
              if key in seen: continue
              seen.add(key)
              out.append(x)
              if len(out) >= MAX_ITEMS: break

          payload = {"generatedAt": datetime.now(timezone.utc).isoformat(), "items": out}
          with open("feed.json", "w", encoding="utf-8") as f:
              json.dump(payload, f, ensure_ascii=False, indent=2)
          print(f"Wrote {len(out)} items")
          PY

      - name: Commit feed.json
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Update feed.json"
          file_pattern: "feed.json"
          push_options: "--force"

      - name: Install Python deps
        run: |
          python3 -m pip install --upgrade pip
          pip3 install requests fpdf2 google-api-python-client google-auth trafilatura
          mkdir -p /tmp/fonts
          wget -q "https://github.com/dejavu-fonts/dejavu-fonts/releases/download/version_2_37/dejavu-fonts-ttf-2.37.tar.bz2" -O /tmp/dejavu.tar.bz2
          tar -xjf /tmp/dejavu.tar.bz2 -C /tmp/fonts --strip-components=2 --wildcards "*/ttf/DejaVuSans.ttf" "*/ttf/DejaVuSans-Bold.ttf"
          echo "Fonts ready:" && ls /tmp/fonts/

      - name: Sync feed.json to Notion database (with PDF archive)
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
          GOOGLE_SERVICE_ACCOUNT_JSON: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_JSON }}
          GOOGLE_DRIVE_FOLDER_ID: ${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}
          SCRAPER_API_KEY: ${{ secrets.SCRAPER_API_KEY }}
        run: |
          python3 - << 'PY'
          import os, json, re, time, textwrap, tempfile
          import requests
          import trafilatura
          from fpdf import FPDF
          from fpdf.enums import XPos, YPos
          from google.oauth2 import service_account
          from googleapiclient.discovery import build
          from googleapiclient.http import MediaFileUpload

          FONT_REGULAR = "/tmp/fonts/DejaVuSans.ttf"
          FONT_BOLD    = "/tmp/fonts/DejaVuSans-Bold.ttf"

          def clean_secret(s):
              return (s or "").strip().strip('"').strip("'").replace("\n","").replace("\r","").replace("\t","").strip()

          def fetch_full_article(url, scraper_key):
              """
              Fetch real article via ScraperAPI (works on real URLs, not Google News).
              Falls back to direct trafilatura fetch.
              """
              # Try ScraperAPI first
              if scraper_key:
                  try:
                      resp = requests.get(
                          "https://api.scraperapi.com",
                          params={"api_key": scraper_key, "url": url, "render": "false"},
                          timeout=60,
                      )
                      if resp.status_code == 200 and len(resp.text) > 500:
                          text = trafilatura.extract(
                              resp.text,
                              include_comments=False,
                              include_tables=False,
                              favor_recall=True,
                          ) or ""
                          if text.strip():
                              return text.strip()
                  except Exception as e:
                      print(f"  ScraperAPI error: {e}")

              # Fallback: direct trafilatura fetch
              try:
                  downloaded = trafilatura.fetch_url(url)
                  if downloaded:
                      text = trafilatura.extract(
                          downloaded,
                          include_comments=False,
                          include_tables=False,
                          favor_recall=True,
                      ) or ""
                      return text.strip()
              except Exception as e:
                  print(f"  trafilatura error: {e}")

              return ""

          def build_drive_service(sa_json):
              info = json.loads(sa_json)
              creds = service_account.Credentials.from_service_account_info(
                  info, scopes=["https://www.googleapis.com/auth/drive.file"])
              return build("drive", "v3", credentials=creds, cache_discovery=False)

          def add_text(pdf, text, bold=False, size=10, color=(50,50,50), width=88, lh=6):
              pdf.set_font("DejaVuBold" if bold else "DejaVu", "", size)
              pdf.set_text_color(*color)
              for para in text.split("\n"):
                  para = para.strip()
                  if not para:
                      pdf.ln(3)
                      continue
                  for line in textwrap.wrap(para, width=width) or [""]:
                      pdf.cell(0, lh, line, new_x=XPos.LMARGIN, new_y=YPos.NEXT)

          def make_pdf(item, article_text):
              pdf = FPDF()
              pdf.set_auto_page_break(auto=True, margin=15)
              pdf.add_font("DejaVu", "", FONT_REGULAR)
              pdf.add_font("DejaVuBold", "", FONT_BOLD)
              pdf.add_page()

              # Header
              pdf.set_fill_color(20, 40, 80)
              pdf.rect(0, 0, 210, 20, "F")
              pdf.set_font("DejaVuBold", "", 12)
              pdf.set_text_color(255, 255, 255)
              pdf.set_xy(8, 5)
              pdf.cell(0, 10, "GFV News Archive", new_x=XPos.LMARGIN, new_y=YPos.NEXT)
              pdf.ln(8)

              # Headline
              add_text(pdf, item.get("headline") or "Untitled",
                       bold=True, size=14, color=(15,15,15), width=70, lh=8)
              pdf.ln(4)

              # Meta
              pdf.set_font("DejaVu", "", 9)
              pdf.set_text_color(120, 120, 120)
              pdf.cell(0, 5, f"{item.get('source','')}  ·  {item.get('pubDate','')}", new_x=XPos.LMARGIN, new_y=YPos.NEXT)
              pdf.cell(0, 5, f"Query: {item.get('query','')}", new_x=XPos.LMARGIN, new_y=YPos.NEXT)
              pdf.ln(5)

              # URL
              pdf.set_draw_color(220, 220, 220)
              pdf.line(10, pdf.get_y(), 200, pdf.get_y())
              pdf.ln(4)
              add_text(pdf, "Article URL:", bold=True, size=9, color=(30,30,30), lh=6)
              pdf.set_font("DejaVu", "", 9)
              pdf.set_text_color(0, 70, 160)
              for chunk in textwrap.wrap(item.get("link",""), width=100):
                  pdf.cell(0, 5, chunk, new_x=XPos.LMARGIN, new_y=YPos.NEXT)
              pdf.ln(5)

              # Content
              pdf.set_draw_color(220, 220, 220)
              pdf.line(10, pdf.get_y(), 200, pdf.get_y())
              pdf.ln(4)

              # Use full article if available, then NewsAPI content, then snippet
              body = article_text or item.get("content","").strip() or item.get("snippet","").strip()
              label = "Full article:" if article_text else ("Article preview:" if item.get("content") else "Snippet:")

              if body:
                  add_text(pdf, label, bold=True, size=10, color=(30,30,30), lh=6)
                  pdf.ln(2)
                  add_text(pdf, body, size=10, color=(40,40,40), width=92, lh=6)
              else:
                  add_text(pdf, "No content available. Visit the article URL above.",
                           size=9, color=(130,130,130), lh=6)

              # Footer
              pdf.set_y(-16)
              pdf.set_draw_color(220, 220, 220)
              pdf.line(10, pdf.get_y(), 200, pdf.get_y())
              pdf.ln(2)
              pdf.set_font("DejaVu", "", 7)
              pdf.set_text_color(170, 170, 170)
              pdf.cell(0, 5, f"GFV News Monitor  ·  {time.strftime('%Y-%m-%d %H:%M UTC', time.gmtime())}",
                       new_x=XPos.LMARGIN, new_y=YPos.NEXT)

              tmp = tempfile.NamedTemporaryFile(suffix=".pdf", delete=False)
              pdf.output(tmp.name)
              return tmp.name

          def upload_to_drive(service, pdf_path, filename, folder_id):
              meta = {"name": filename, "parents": [folder_id]}
              media = MediaFileUpload(pdf_path, mimetype="application/pdf", resumable=False)
              f = service.files().create(
                  body=meta, media_body=media, fields="id",
                  supportsAllDrives=True).execute()
              file_id = f["id"]
              service.permissions().create(
                  fileId=file_id,
                  body={"type": "anyone", "role": "reader"},
                  supportsAllDrives=True).execute()
              return f"https://drive.google.com/file/d/{file_id}/view"

          # ── main ─────────────────────────────────────────────────────────────

          token        = clean_secret(os.environ.get("NOTION_TOKEN"))
          db_raw       = clean_secret(os.environ.get("NOTION_DATABASE_ID")).replace("-","")
          sa_json      = os.environ.get("GOOGLE_SERVICE_ACCOUNT_JSON","").strip()
          drive_folder = clean_secret(os.environ.get("GOOGLE_DRIVE_FOLDER_ID"))
          scraper_key  = clean_secret(os.environ.get("SCRAPER_API_KEY"))

          if not token: raise RuntimeError("Missing NOTION_TOKEN")
          if not re.fullmatch(r"[0-9a-fA-F]{32}", db_raw):
              raise RuntimeError(f"NOTION_DATABASE_ID invalid: length={len(db_raw)}")
          if not sa_json: raise RuntimeError("Missing GOOGLE_SERVICE_ACCOUNT_JSON")
          if not drive_folder: raise RuntimeError("Missing GOOGLE_DRIVE_FOLDER_ID")

          db_id = f"{db_raw[0:8]}-{db_raw[8:12]}-{db_raw[12:16]}-{db_raw[16:20]}-{db_raw[20:32]}"
          notion_headers = {
              "Authorization": f"Bearer {token}",
              "Notion-Version": "2022-06-28",
              "Content-Type": "application/json",
          }

          pre = requests.get(f"https://api.notion.com/v1/databases/{db_id}",
                             headers=notion_headers, timeout=30)
          if pre.status_code != 200:
              raise RuntimeError(f"Notion preflight failed: {pre.status_code}\n{pre.text}")

          drive_svc = build_drive_service(sa_json)

          with open("feed.json","r",encoding="utf-8") as f:
              items = json.load(f).get("items",[])

          existing_urls = set()
          cursor = None
          for _ in range(50):
              body = {"page_size": 100}
              if cursor: body["start_cursor"] = cursor
              r = requests.post(f"https://api.notion.com/v1/databases/{db_id}/query",
                                headers=notion_headers, json=body, timeout=30)
              if r.status_code != 200:
                  raise RuntimeError(f"Notion query failed: {r.status_code}\n{r.text}")
              data = r.json()
              for page in data.get("results",[]):
                  u = (page.get("properties",{}).get("URL",{}) or {}).get("url")
                  if u: existing_urls.add(u)
              if not data.get("has_more"): break
              cursor = data.get("next_cursor")

          created = 0
          for it in items:
              url = it.get("link")
              if not url or url in existing_urls:
                  continue

              title  = (it.get("headline") or "Untitled")[:180]
              source = (it.get("source") or "")[:200]
              query  = (it.get("query") or "")[:200]
              iso    = it.get("pubDateISO")

              print(f"\n  Processing: {title[:70]}...")

              # Fetch full article from real URL via ScraperAPI
              article_text = fetch_full_article(url, scraper_key)
              print(f"  Text: {len(article_text)} chars" if article_text else "  Text: none (using NewsAPI content/snippet)")

              pdf_path = make_pdf(it, article_text)

              safe_title = re.sub(r'[\\/*?:"<>|]', "", title)[:80]
              filename = f"{(iso or '')[:10]} - {safe_title}.pdf"
              try:
                  drive_url = upload_to_drive(drive_svc, pdf_path, filename, drive_folder)
              except Exception as e:
                  print(f"  Drive upload failed: {e}")
                  drive_url = None
              finally:
                  os.unlink(pdf_path)

              props = {
                  "Name":   {"title": [{"text": {"content": title}}]},
                  "URL":    {"url": url},
                  "Source": {"rich_text": [{"text": {"content": source}}]} if source else {"rich_text": []},
                  "Query":  {"rich_text": [{"text": {"content": query}}]} if query else {"rich_text": []},
              }
              if isinstance(iso, str) and "T" in iso:
                  props["DATE"] = {"date": {"start": iso}}
              if drive_url:
                  props["PDF"] = {"url": drive_url}

              c = requests.post("https://api.notion.com/v1/pages",
                                headers=notion_headers,
                                json={"parent": {"database_id": db_id}, "properties": props},
                                timeout=30)
              if c.status_code != 200:
                  raise RuntimeError(f"Notion create failed: {c.status_code}\n{c.text}")

              existing_urls.add(url)
              created += 1
              print(f"  ✓ Row created | PDF: {drive_url or 'none'}")
              time.sleep(0.5)

          print(f"\nDone: created={created}, existing={len(existing_urls)}")
          PY
