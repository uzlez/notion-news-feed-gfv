name: Update feed.json (GFV mentions)

on:
  workflow_dispatch:
  schedule:
    - cron: "*/30 * * * *"

concurrency:
  group: notion-news-feed-gfv
  cancel-in-progress: true

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Fetch Google News RSS and write feed.json
        run: |
          python3 - << 'PY'
          import json, re, urllib.parse, urllib.request
          import xml.etree.ElementTree as ET
          from datetime import datetime, timezone, timedelta
          from email.utils import parsedate_to_datetime

          MAX_ITEMS = 200
          MAX_AGE_DAYS = 180

          HL = "en-GB"
          GL = "GB"
          CEID = "GB:en"

          QUERIES = [
              '"Green Flag Ventures"',
              '"Justin Zeefe"',
              '"Deborah Fairlamb"',
              'GFV -("gross floor value") -("good faith violation") -("gross floor")',
              '("Green Flag Ventures" OR "Justin Zeefe" OR "Deborah Fairlamb" OR GFV) (venture OR VC OR fund OR investment OR portfolio OR backs OR backed OR led OR seed)',
          ]

          cutoff = datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)
          cutoff_date = cutoff.date().isoformat()

          def clean_html(s: str) -> str:
              if not s:
                  return ""
              s = re.sub(r"<br\s*/?>", " ", s, flags=re.I)
              s = re.sub(r"<[^>]+>", "", s)
              s = re.sub(r"\s+", " ", s).strip()
              return s

          def parse_pubdate(s: str):
              try:
                  dt = parsedate_to_datetime(s)
                  if dt.tzinfo is None:
                      dt = dt.replace(tzinfo=timezone.utc)
                  return dt.astimezone(timezone.utc)
              except Exception:
                  return None

          items = []
          for q in QUERIES:
              q2 = f"{q} after:{cutoff_date}"
              url = "https://news.google.com/rss/search?" + urllib.parse.urlencode({
                  "q": q2,
                  "hl": HL,
                  "gl": GL,
                  "ceid": CEID,
              })

              try:
                  with urllib.request.urlopen(url, timeout=30) as resp:
                      xml = resp.read()
              except Exception as e:
                  print("Fetch failed:", q, e)
                  continue

              root = ET.fromstring(xml)
              channel = root.find("channel")
              if channel is None:
                  continue

              for it in channel.findall("item"):
                  title = (it.findtext("title") or "").strip()
                  link = (it.findtext("link") or "").strip()
                  pub = (it.findtext("pubDate") or "").strip()
                  src_el = it.find("source")
                  source = (src_el.text or "").strip() if src_el is not None and src_el.text else ""
                  desc = clean_html((it.findtext("description") or "").strip())

                  dt = parse_pubdate(pub)
                  if not dt or dt < cutoff:
                      continue

                  items.append({
                      "headline": title,
                      "source": source,
                      "link": link,
                      "pubDate": dt.strftime("%a, %d %b %Y %H:%M:%S GMT"),
                      "pubDateISO": dt.isoformat(),
                      "snippet": desc,
                      "query": q,
                  })

          seen = set()
          out = []
          for x in sorted(items, key=lambda z: z["pubDateISO"], reverse=True):
              key = x["link"] or (x["headline"] + "|" + x["pubDate"])
              if key in seen:
                  continue
              seen.add(key)
              out.append(x)
              if len(out) >= MAX_ITEMS:
                  break

          payload = {"generatedAt": datetime.now(timezone.utc).isoformat(), "items": out}
          with open("feed.json", "w", encoding="utf-8") as f:
              json.dump(payload, f, ensure_ascii=False, indent=2)

          print("Wrote", len(out), "items")
          PY

      - name: Commit feed.json
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Update feed.json"
          file_pattern: "feed.json"

      - name: Install Python deps
        run: |
          python3 -m pip install --upgrade pip
          pip3 install requests

      - name: Sync feed.json to Notion database
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
        run: |
          python3 - << 'PY'
          import os, json, re, time
          import requests

          def clean_secret(s: str) -> str:
              s = (s or "")
              s = s.strip()
              s = s.strip('"').strip("'")
              s = s.replace("\n", "").replace("\r", "").replace("\t", "").strip()
              return s

          token = clean_secret(os.environ.get("NOTION_TOKEN"))
          db_raw = clean_secret(os.environ.get("NOTION_DATABASE_ID")).replace("-", "")

          if not token:
              raise RuntimeError("Missing GitHub secret: NOTION_TOKEN")
          if not db_raw:
              raise RuntimeError("Missing GitHub secret: NOTION_DATABASE_ID")

          # Validate DB id is 32 hex
          if not re.fullmatch(r"[0-9a-fA-F]{32}", db_raw):
              raise RuntimeError(
                  f"NOTION_DATABASE_ID is not a 32-hex database id after cleaning. "
                  f"Got length={len(db_raw)}. Use ONLY the id (e.g. 310ebf7ff2a68067999bf025950fbc70), no URL, no v=."
              )

          # Canonical UUID with hyphens (Notion always accepts this)
          db_id = f"{db_raw[0:8]}-{db_raw[8:12]}-{db_raw[12:16]}-{db_raw[16:20]}-{db_raw[20:32]}"

          headers = {
              "Authorization": f"Bearer {token}",
              "Notion-Version": "2022-06-28",
              "Content-Type": "application/json",
          }

          # Preflight: confirm this id is a DATABASE and that the integration can access it
          pre = requests.get(f"https://api.notion.com/v1/databases/{db_id}", headers=headers, timeout=30)
          if pre.status_code != 200:
              raise RuntimeError(
                  f"Notion preflight failed (GET /databases/<id>): status={pre.status_code}\n{pre.text}\n"
                  f"Fix: ensure the integration has access to the database, and NOTION_DATABASE_ID is the database id."
              )

          with open("feed.json", "r", encoding="utf-8") as f:
              feed = json.load(f)
          items = feed.get("items", [])

          # Pull existing URLs from Notion (few calls)
          existing_urls = set()
          start_cursor = None
          for _ in range(50):
              payload = {"page_size": 100}
              if start_cursor:
                  payload["start_cursor"] = start_cursor
              r = requests.post(
                  f"https://api.notion.com/v1/databases/{db_id}/query",
                  headers=headers,
                  json=payload,
                  timeout=30
              )
              if r.status_code != 200:
                  raise RuntimeError(f"Notion query failed: status={r.status_code}\n{r.text}")
              data = r.json()
              for page in data.get("results", []):
                  u = (page.get("properties", {}).get("URL", {}) or {}).get("url")
                  if u:
                      existing_urls.add(u)
              if not data.get("has_more"):
                  break
              start_cursor = data.get("next_cursor")

          created = 0
          for it in items:
              url = it.get("link")
              if not url or url in existing_urls:
                  continue

              title = (it.get("headline") or "Untitled")[:180]
              source = (it.get("source") or "")[:200]
              query = (it.get("query") or "")[:200]
              iso = it.get("pubDateISO")

              # Property names must match your DB exactly:
              # Name (title), URL (url), DATE (date), Source (rich_text), Query (rich_text)
              props = {
                  "Name": {"title": [{"text": {"content": title}}]},
                  "URL": {"url": url},
                  "Source": {"rich_text": [{"text": {"content": source}}]} if source else {"rich_text": []},
                  "Query": {"rich_text": [{"text": {"content": query}}]} if query else {"rich_text": []},
              }
              if isinstance(iso, str) and "T" in iso:
                  props["DATE"] = {"date": {"start": iso}}

              create_payload = {"parent": {"database_id": db_id}, "properties": props}
              c = requests.post("https://api.notion.com/v1/pages", headers=headers, json=create_payload, timeout=30)
              if c.status_code != 200:
                  raise RuntimeError(f"Notion create failed: status={c.status_code}\n{c.text}")

              existing_urls.add(url)
              created += 1
              time.sleep(0.35)

          print(f"Notion sync complete: created={created}, existing={len(existing_urls)}")
          PY
