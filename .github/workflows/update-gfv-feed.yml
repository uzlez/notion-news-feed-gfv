name: Update feed.json (GFV mentions)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"

concurrency:
  group: notion-news-feed-gfv
  cancel-in-progress: true

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Fetch Google News RSS and write feed.json
        run: |
          python3 - << 'PY'
          import json, re, urllib.parse, urllib.request
          import xml.etree.ElementTree as ET
          from datetime import datetime, timezone, timedelta
          from email.utils import parsedate_to_datetime

          MAX_ITEMS = 200
          MAX_AGE_DAYS = 180
          HL = "en-GB"
          GL = "GB"
          CEID = "GB:en"

          QUERIES = [
              '"Green Flag Ventures"',
              '"Justin Zeefe"',
              '"Deborah Fairlamb"',
              'GFV -("gross floor value") -("good faith violation") -("gross floor")',
              '("Green Flag Ventures" OR "Justin Zeefe" OR "Deborah Fairlamb" OR GFV) (venture OR VC OR fund OR investment OR portfolio OR backs OR backed OR led OR seed)',
          ]

          cutoff = datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)
          cutoff_date = cutoff.date().isoformat()

          def clean_html(s):
              if not s: return ""
              s = re.sub(r"<br\s*/?>", " ", s, flags=re.I)
              s = re.sub(r"<[^>]+>", "", s)
              s = re.sub(r"&nbsp;", " ", s)
              s = re.sub(r"&amp;", "&", s)
              s = re.sub(r"&lt;", "<", s)
              s = re.sub(r"&gt;", ">", s)
              s = re.sub(r"&quot;", '"', s)
              s = re.sub(r"&#?\w+;", "", s)
              return re.sub(r"\s+", " ", s).strip()

          def parse_pubdate(s):
              try:
                  dt = parsedate_to_datetime(s)
                  if dt.tzinfo is None:
                      dt = dt.replace(tzinfo=timezone.utc)
                  return dt.astimezone(timezone.utc)
              except Exception:
                  return None

          items = []
          for q in QUERIES:
              q2 = f"{q} after:{cutoff_date}"
              url = "https://news.google.com/rss/search?" + urllib.parse.urlencode({
                  "q": q2, "hl": HL, "gl": GL, "ceid": CEID,
              })
              try:
                  with urllib.request.urlopen(url, timeout=30) as resp:
                      xml = resp.read()
              except Exception as e:
                  print(f"Fetch failed for '{q}': {e}")
                  continue

              root = ET.fromstring(xml)
              channel = root.find("channel")
              if channel is None: continue

              for it in channel.findall("item"):
                  title   = clean_html((it.findtext("title") or "").strip())
                  link    = (it.findtext("link") or "").strip()
                  pub     = (it.findtext("pubDate") or "").strip()
                  src_el  = it.find("source")
                  source  = clean_html((src_el.text or "").strip()) if src_el is not None else ""
                  src_url = (src_el.get("url") or "").strip() if src_el is not None else ""
                  desc    = clean_html((it.findtext("description") or "").strip())

                  dt = parse_pubdate(pub)
                  if not dt or dt < cutoff: continue

                  # Clean up title — Google News appends " - Source Name" at the end
                  if source and title.endswith(f" - {source}"):
                      title = title[: -(len(source) + 3)].strip()

                  items.append({
                      "headline": title,
                      "source": source,
                      "source_url": src_url,
                      "link": link,
                      "pubDate": dt.strftime("%a, %d %b %Y %H:%M:%S GMT"),
                      "pubDateISO": dt.isoformat(),
                      "snippet": desc,
                      "query": q,
                  })

          # ── PATCHED: deduplicate by BOTH url AND normalised title ──────────────
          # Google News returns the same article with multiple redirect URLs
          # (e.g. rss...B?oc=5, rss...n?oc=5) so URL-only dedup misses them.
          seen_urls   = set()
          seen_titles = set()
          out = []
          for x in sorted(items, key=lambda z: z["pubDateISO"], reverse=True):
              url_key   = x["link"]
              title_key = re.sub(r"\s+", " ", x["headline"].lower()).strip()
              if url_key in seen_urls or title_key in seen_titles:
                  continue
              seen_urls.add(url_key)
              seen_titles.add(title_key)
              out.append(x)
              if len(out) >= MAX_ITEMS:
                  break
          # ── END PATCH ───────────────────────────────────────────────────────────

          payload = {"generatedAt": datetime.now(timezone.utc).isoformat(), "items": out}
          with open("feed.json", "w", encoding="utf-8") as f:
              json.dump(payload, f, ensure_ascii=False, indent=2)
          print(f"Wrote {len(out)} items")
          PY

      - name: Commit feed.json
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Update feed.json"
          file_pattern: "feed.json"
          push_options: "--force"

      - name: Install Python deps
        run: |
          python3 -m pip install --upgrade pip
          pip3 install requests fpdf2 google-api-python-client google-auth trafilatura
          mkdir -p /tmp/fonts
          wget -q "https://github.com/dejavu-fonts/dejavu-fonts/releases/download/version_2_37/dejavu-fonts-ttf-2.37.tar.bz2" -O /tmp/dejavu.tar.bz2
          tar -xjf /tmp/dejavu.tar.bz2 -C /tmp/fonts --strip-components=2 --wildcards "*/ttf/DejaVuSans.ttf" "*/ttf/DejaVuSans-Bold.ttf"
          echo "Fonts ready:" && ls /tmp/fonts/

      - name: Sync feed.json to Notion database (with PDF archive)
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
          GOOGLE_SERVICE_ACCOUNT_JSON: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_JSON }}
          GOOGLE_DRIVE_FOLDER_ID: ${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}
          SCRAPER_API_KEY: ${{ secrets.SCRAPER_API_KEY }}
        run: |
          python3 - << 'PY'
          import os, json, re, time, textwrap, tempfile
          import requests
          import trafilatura
          from fpdf import FPDF
          from fpdf.enums import XPos, YPos
          from google.oauth2 import service_account
          from googleapiclient.discovery import build
          from googleapiclient.http import MediaFileUpload

          FONT_REGULAR = "/tmp/fonts/DejaVuSans.ttf"
          FONT_BOLD    = "/tmp/fonts/DejaVuSans-Bold.ttf"

          def clean_secret(s):
              return (s or "").strip().strip('"').strip("'").replace("\n","").replace("\r","").replace("\t","").strip()

          def try_fetch_article(source_url, headline, scraper_key):
              if not source_url or not scraper_key:
                  return "", ""
              domain = re.sub(r"https?://", "", source_url).rstrip("/")
              keywords = " ".join(headline.split()[:6])
              search_url = f"https://www.google.com/search?q=site:{domain}+{requests.utils.quote(keywords)}"
              try:
                  resp = requests.get(
                      "https://api.scraperapi.com",
                      params={"api_key": scraper_key, "url": search_url, "render": "false"},
                      timeout=45,
                  )
                  if resp.status_code != 200:
                      return "", ""
                  matches = re.findall(
                      rf'href="(https?://{re.escape(domain)}/[^"]+)"',
                      resp.text
                  )
                  if not matches:
                      matches = re.findall(r'/url\?q=(https?://[^&"]+)', resp.text)
                  if not matches:
                      return "", ""
                  article_url = matches[0]
                  article_url = urllib.parse.unquote(article_url) if "%3A" in article_url else article_url
                  resp2 = requests.get(
                      "https://api.scraperapi.com",
                      params={"api_key": scraper_key, "url": article_url, "render": "false"},
                      timeout=45,
                  )
                  if resp2.status_code == 200 and len(resp2.text) > 500:
                      text = trafilatura.extract(
                          resp2.text,
                          include_comments=False,
                          include_tables=False,
                          favor_recall=True,
                      ) or ""
                      if text.strip():
                          return article_url, text.strip()
              except Exception as e:
                  print(f"  Article fetch error: {e}")
              return "", ""

          def build_drive_service(sa_json):
              info = json.loads(sa_json)
              creds = service_account.Credentials.from_service_account_info(
                  info, scopes=["https://www.googleapis.com/auth/drive.file"])
              return build("drive", "v3", credentials=creds, cache_discovery=False)

          def add_text(pdf, text, bold=False, size=10, color=(50,50,50), width=88, lh=6):
              pdf.set_font("DejaVuBold" if bold else "DejaVu", "", size)
              pdf.set_text_color(*color)
              for para in text.split("\n"):
                  para = para.strip()
                  if not para:
                      pdf.ln(3)
                      continue
                  for line in textwrap.wrap(para, width=width) or [""]:
                      pdf.cell(0, lh, line, new_x=XPos.LMARGIN, new_y=YPos.NEXT)

          def make_pdf(item, real_url, article_text):
              pdf = FPDF()
              pdf.set_auto_page_break(auto=True, margin=15)
              pdf.add_font("DejaVu", "", FONT_REGULAR)
              pdf.add_font("DejaVuBold", "", FONT_BOLD)
              pdf.add_page()

              pdf.set_fill_color(20, 40, 80)
              pdf.rect(0, 0, 210, 22, "F")
              pdf.set_font("DejaVuBold", "", 11)
              pdf.set_text_color(255, 255, 255)
              pdf.set_xy(10, 4)
              pdf.cell(130, 7, "GFV News Archive", new_x=XPos.RIGHT, new_y=YPos.TOP)
              pdf.set_font("DejaVu", "", 9)
              pdf.set_text_color(180, 200, 230)
              pdf.set_xy(140, 4)
              pdf.cell(60, 7, item.get("pubDate","")[:16], new_x=XPos.LMARGIN, new_y=YPos.NEXT)
              pdf.set_font("DejaVu", "", 9)
              pdf.set_text_color(160, 185, 220)
              pdf.set_xy(10, 13)
              pdf.cell(0, 6, item.get("source",""), new_x=XPos.LMARGIN, new_y=YPos.NEXT)
              pdf.ln(10)

              add_text(pdf, item.get("headline") or "Untitled",
                       bold=True, size=16, color=(10,10,40), width=68, lh=9)
              pdf.ln(6)

              pdf.set_draw_color(20, 40, 80)
              pdf.set_line_width(0.5)
              pdf.line(10, pdf.get_y(), 50, pdf.get_y())
              pdf.set_line_width(0.2)
              pdf.ln(6)

              display_url = real_url if real_url else item.get("source_url","")
              if display_url:
                  pdf.set_font("DejaVuBold", "", 8)
                  pdf.set_text_color(80, 80, 80)
                  pdf.cell(18, 5, "Source:", new_x=XPos.RIGHT, new_y=YPos.TOP)
                  pdf.set_font("DejaVu", "", 8)
                  pdf.set_text_color(0, 70, 160)
                  url_line = display_url[:100]
                  pdf.cell(0, 5, url_line, new_x=XPos.LMARGIN, new_y=YPos.NEXT)
                  pdf.ln(4)

              pdf.set_draw_color(230, 230, 230)
              pdf.line(10, pdf.get_y(), 200, pdf.get_y())
              pdf.ln(5)

              if article_text:
                  add_text(pdf, article_text, size=10, color=(30,30,30), width=92, lh=6)
              else:
                  snippet = (item.get("snippet") or "").strip()
                  if snippet:
                      pdf.set_fill_color(248, 249, 252)
                      y_before = pdf.get_y()
                      pdf.set_font("DejaVu", "", 10)
                      pdf.set_text_color(60, 60, 60)
                      wrapped = []
                      for para in snippet.split("\n"):
                          para = para.strip()
                          if para:
                              wrapped.extend(textwrap.wrap(para, width=88) or [""])
                          else:
                              wrapped.append("")
                      box_h = len(wrapped) * 6 + 8
                      pdf.rect(8, y_before - 2, 194, box_h, "F")
                      for line in wrapped:
                          pdf.cell(0, 6, line, new_x=XPos.LMARGIN, new_y=YPos.NEXT)
                  else:
                      pdf.set_font("DejaVu", "", 9)
                      pdf.set_text_color(160, 160, 160)
                      pdf.cell(0, 6, "No preview available — open the source URL to read the full article.",
                               new_x=XPos.LMARGIN, new_y=YPos.NEXT)

              pdf.ln(6)
              pdf.set_font("DejaVu", "", 7)
              pdf.set_text_color(160, 160, 160)
              pdf.cell(0, 5, f"Matched query: {item.get('query','')}",
                       new_x=XPos.LMARGIN, new_y=YPos.NEXT)

              pdf.set_y(-14)
              pdf.set_draw_color(200, 200, 200)
              pdf.line(10, pdf.get_y(), 200, pdf.get_y())
              pdf.ln(2)
              pdf.set_font("DejaVu", "", 7)
              pdf.set_text_color(180, 180, 180)
              pdf.cell(0, 5,
                       f"GFV News Monitor  ·  Generated {time.strftime('%Y-%m-%d %H:%M UTC', time.gmtime())}",
                       new_x=XPos.LMARGIN, new_y=YPos.NEXT)

              tmp = tempfile.NamedTemporaryFile(suffix=".pdf", delete=False)
              pdf.output(tmp.name)
              return tmp.name

          def upload_to_drive(service, pdf_path, filename, folder_id):
              meta = {"name": filename, "parents": [folder_id]}
              media = MediaFileUpload(pdf_path, mimetype="application/pdf", resumable=False)
              f = service.files().create(
                  body=meta, media_body=media, fields="id",
                  supportsAllDrives=True).execute()
              file_id = f["id"]
              service.permissions().create(
                  fileId=file_id,
                  body={"type": "anyone", "role": "reader"},
                  supportsAllDrives=True).execute()
              return f"https://drive.google.com/file/d/{file_id}/view"

          # ── main ─────────────────────────────────────────────────────────────

          import urllib.parse

          token        = clean_secret(os.environ.get("NOTION_TOKEN"))
          db_raw       = clean_secret(os.environ.get("NOTION_DATABASE_ID")).replace("-","")
          sa_json      = os.environ.get("GOOGLE_SERVICE_ACCOUNT_JSON","").strip()
          drive_folder = clean_secret(os.environ.get("GOOGLE_DRIVE_FOLDER_ID"))
          scraper_key  = clean_secret(os.environ.get("SCRAPER_API_KEY"))

          if not token: raise RuntimeError("Missing NOTION_TOKEN")
          if not re.fullmatch(r"[0-9a-fA-F]{32}", db_raw):
              raise RuntimeError(f"NOTION_DATABASE_ID invalid: length={len(db_raw)}")
          if not sa_json: raise RuntimeError("Missing GOOGLE_SERVICE_ACCOUNT_JSON")
          if not drive_folder: raise RuntimeError("Missing GOOGLE_DRIVE_FOLDER_ID")

          db_id = f"{db_raw[0:8]}-{db_raw[8:12]}-{db_raw[12:16]}-{db_raw[16:20]}-{db_raw[20:32]}"
          notion_headers = {
              "Authorization": f"Bearer {token}",
              "Notion-Version": "2022-06-28",
              "Content-Type": "application/json",
          }

          pre = requests.get(f"https://api.notion.com/v1/databases/{db_id}",
                             headers=notion_headers, timeout=30)
          if pre.status_code != 200:
              raise RuntimeError(f"Notion preflight failed: {pre.status_code}\n{pre.text}")

          drive_svc = build_drive_service(sa_json)

          with open("feed.json","r",encoding="utf-8") as f:
              items = json.load(f).get("items",[])

          # ── PATCHED: collect existing URLs AND titles from Notion ────────────
          existing_urls   = set()
          existing_titles = set()
          cursor = None
          for _ in range(50):
              body = {"page_size": 100}
              if cursor: body["start_cursor"] = cursor
              r = requests.post(f"https://api.notion.com/v1/databases/{db_id}/query",
                                headers=notion_headers, json=body, timeout=30)
              if r.status_code != 200:
                  raise RuntimeError(f"Notion query failed: {r.status_code}\n{r.text}")
              data = r.json()
              for page in data.get("results",[]):
                  props = page.get("properties", {})
                  # Collect existing URL
                  u = (props.get("URL", {}) or {}).get("url")
                  if u:
                      existing_urls.add(u)
                  # Collect existing title (normalised)
                  title_parts = (props.get("Name", {}) or {}).get("title") or []
                  if title_parts:
                      t = (title_parts[0].get("plain_text") or "").lower().strip()
                      t = re.sub(r"\s+", " ", t)
                      if t:
                          existing_titles.add(t)
              if not data.get("has_more"): break
              cursor = data.get("next_cursor")

          print(f"Existing in Notion: {len(existing_urls)} URLs, {len(existing_titles)} titles")
          # ── END PATCH ────────────────────────────────────────────────────────

          created = 0
          for it in items:
              url = it.get("link")
              title = (it.get("headline") or "Untitled")[:180]

              # ── PATCHED: skip if URL or title already exists ─────────────────
              title_key = re.sub(r"\s+", " ", title.lower()).strip()
              if not url or url in existing_urls or title_key in existing_titles:
                  continue
              # ── END PATCH ────────────────────────────────────────────────────

              source     = (it.get("source") or "")[:200]
              source_url = it.get("source_url","")
              query      = (it.get("query") or "")[:200]
              iso        = it.get("pubDateISO")

              print(f"\n  Processing: {title[:70]}...")

              real_url, article_text = try_fetch_article(source_url, title, scraper_key)
              if article_text:
                  print(f"  ✓ Got {len(article_text)} chars from {real_url[:60]}")
              else:
                  print(f"  → Using snippet/card layout")

              pdf_path = make_pdf(it, real_url, article_text)

              safe_title = re.sub(r'[\\/*?:"<>|]', "", title)[:80]
              filename = f"{(iso or '')[:10]} - {safe_title}.pdf"
              try:
                  drive_url = upload_to_drive(drive_svc, pdf_path, filename, drive_folder)
              except Exception as e:
                  print(f"  Drive upload failed: {e}")
                  drive_url = None
              finally:
                  os.unlink(pdf_path)

              props = {
                  "Name":   {"title": [{"text": {"content": title}}]},
                  "URL":    {"url": url},
                  "Source": {"rich_text": [{"text": {"content": source}}]} if source else {"rich_text": []},
                  "Query":  {"rich_text": [{"text": {"content": query}}]} if query else {"rich_text": []},
              }
              if isinstance(iso, str) and "T" in iso:
                  props["DATE"] = {"date": {"start": iso}}
              if drive_url:
                  props["PDF"] = {"url": drive_url}

              c = requests.post("https://api.notion.com/v1/pages",
                                headers=notion_headers,
                                json={"parent": {"database_id": db_id}, "properties": props},
                                timeout=30)
              if c.status_code != 200:
                  raise RuntimeError(f"Notion create failed: {c.status_code}\n{c.text}")

              # ── PATCHED: track both URL and title after insert ───────────────
              existing_urls.add(url)
              existing_titles.add(title_key)
              # ── END PATCH ────────────────────────────────────────────────────

              created += 1
              print(f"  ✓ Row created | PDF: {drive_url or 'none'}")
              time.sleep(0.5)

          print(f"\nDone: created={created}, existing={len(existing_urls)}")
          PY
