name: Update feed.json (GFV mentions)

on:
  workflow_dispatch:
  schedule:
    - cron: "*/30 * * * *"

concurrency:
  group: notion-news-feed-gfv
  cancel-in-progress: true

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Fetch Google News RSS and write feed.json
        run: |
          python3 - << 'PY'
          import json, re, urllib.parse, urllib.request
          import xml.etree.ElementTree as ET
          from datetime import datetime, timezone, timedelta
          from email.utils import parsedate_to_datetime

          MAX_ITEMS = 200
          MAX_AGE_DAYS = 180

          HL = "en-GB"
          GL = "GB"
          CEID = "GB:en"

          QUERIES = [
              '"Green Flag Ventures"',
              '"Justin Zeefe"',
              '"Deborah Fairlamb"',
              'GFV -("gross floor value") -("good faith violation") -("gross floor")',
              '("Green Flag Ventures" OR "Justin Zeefe" OR "Deborah Fairlamb" OR GFV) (venture OR VC OR fund OR investment OR portfolio OR backs OR backed OR led OR seed)',
          ]

          cutoff = datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)
          cutoff_date = cutoff.date().isoformat()

          def clean_html(s: str) -> str:
              if not s:
                  return ""
              s = re.sub(r"<br\s*/?>", " ", s, flags=re.I)
              s = re.sub(r"<[^>]+>", "", s)
              s = re.sub(r"\s+", " ", s).strip()
              return s

          def parse_pubdate(s: str):
              try:
                  dt = parsedate_to_datetime(s)
                  if dt.tzinfo is None:
                      dt = dt.replace(tzinfo=timezone.utc)
                  return dt.astimezone(timezone.utc)
              except Exception:
                  return None

          items = []
          for q in QUERIES:
              q2 = f"{q} after:{cutoff_date}"
              url = "https://news.google.com/rss/search?" + urllib.parse.urlencode({
                  "q": q2,
                  "hl": HL,
                  "gl": GL,
                  "ceid": CEID,
              })

              try:
                  with urllib.request.urlopen(url, timeout=30) as resp:
                      xml = resp.read()
              except Exception as e:
                  print("Fetch failed:", q, e)
                  continue

              root = ET.fromstring(xml)
              channel = root.find("channel")
              if channel is None:
                  continue

              for it in channel.findall("item"):
                  title = (it.findtext("title") or "").strip()
                  link = (it.findtext("link") or "").strip()
                  pub = (it.findtext("pubDate") or "").strip()
                  src_el = it.find("source")
                  source = (src_el.text or "").strip() if src_el is not None and src_el.text else ""
                  desc = clean_html((it.findtext("description") or "").strip())

                  dt = parse_pubdate(pub)
                  if not dt or dt < cutoff:
                      continue

                  items.append({
                      "headline": title,
                      "source": source,
                      "link": link,
                      "pubDate": dt.strftime("%a, %d %b %Y %H:%M:%S GMT"),
                      "pubDateISO": dt.isoformat(),
                      "snippet": desc,
                      "query": q,
                  })

          # De-dupe by link (fallback to headline+pubDate)
          seen = set()
          out = []
          for x in sorted(items, key=lambda z: z["pubDateISO"], reverse=True):
              key = x["link"] or (x["headline"] + "|" + x["pubDate"])
              if key in seen:
                  continue
              seen.add(key)
              out.append(x)
              if len(out) >= MAX_ITEMS:
                  break

          payload = {"generatedAt": datetime.now(timezone.utc).isoformat(), "items": out}
          with open("feed.json", "w", encoding="utf-8") as f:
              json.dump(payload, f, ensure_ascii=False, indent=2)

          print("Wrote", len(out), "items")
          PY

      - name: Commit feed.json
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Update feed.json"
          file_pattern: "feed.json"

      - name: Install Python deps
        run: |
          python3 -m pip install --upgrade pip
          pip3 install requests

      - name: Sync feed.json to Notion database
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
        run: |
          python3 - << 'PY'
          import os, json, time
          import requests

          # --- Read & sanitize secrets ---
          token = (os.environ.get("NOTION_TOKEN") or "").strip()
          token = token.replace("\n", "").replace("\r", "").replace("\t", "")
          db_id = (os.environ.get("NOTION_DATABASE_ID") or "").strip().replace("-", "")
          db_id = db_id.replace("\n", "").replace("\r", "").replace("\t", "")

          if not token:
              raise RuntimeError("Missing GitHub secret: NOTION_TOKEN")
          if not db_id:
              raise RuntimeError("Missing GitHub secret: NOTION_DATABASE_ID")

          # If token still contains spaces/quotes, fail clearly (this is your current issue)
          if any(ch.isspace() for ch in token) or token.startswith(("'", '"')) or token.endswith(("'", '"')):
              raise RuntimeError(
                  "NOTION_TOKEN looks malformed (contains whitespace or quotes). "
                  "Edit the GitHub secret and paste the Notion Internal Integration Token as ONE single line, no quotes."
              )

          headers = {
              "Authorization": f"Bearer {token}",
              "Notion-Version": "2022-06-28",
              "Content-Type": "application/json",
          }

          # --- Load local feed.json produced above ---
          with open("feed.json", "r", encoding="utf-8") as f:
              feed = json.load(f)
          items = feed.get("items", [])

          # --- Fetch existing URLs from Notion efficiently (few API calls) ---
          existing_urls = set()
          start_cursor = None
          max_pages = 30  # safety limit (up to 3000 rows)
          for _ in range(max_pages):
              payload = {"page_size": 100}
              if start_cursor:
                  payload["start_cursor"] = start_cursor

              r = requests.post(
                  f"https://api.notion.com/v1/databases/{db_id}/query",
                  headers=headers,
                  json=payload,
                  timeout=30,
              )
              r.raise_for_status()
              data = r.json()

              for page in data.get("results", []):
                  props = page.get("properties", {})
                  url_prop = props.get("URL", {})
                  u = url_prop.get("url")
                  if u:
                      existing_urls.add(u)

              if not data.get("has_more"):
                  break
              start_cursor = data.get("next_cursor")

          created = 0
          for it in items:
              url = it.get("link")
              if not url or url in existing_urls:
                  continue

              title = (it.get("headline") or "Untitled")[:180]
              source = (it.get("source") or "")[:200]
              query = (it.get("query") or "")[:200]
              iso = it.get("pubDateISO")

              # NOTE: property names must match your Notion DB exactly:
              # Name (title), URL (url), DATE (date), Source (rich_text), Query (rich_text)
              props = {
                  "Name": {"title": [{"text": {"content": title}}]},
                  "URL": {"url": url},
                  "Source": {"rich_text": [{"text": {"content": source}}]} if source else {"rich_text": []},
                  "Query": {"rich_text": [{"text": {"content": query}}]} if query else {"rich_text": []},
              }
              if isinstance(iso, str) and "T" in iso:
                  props["DATE"] = {"date": {"start": iso}}

              payload = {"parent": {"database_id": db_id}, "properties": props}

              r = requests.post(
                  "https://api.notion.com/v1/pages",
                  headers=headers,
                  json=payload,
                  timeout=30,
              )
              r.raise_for_status()

              existing_urls.add(url)
              created += 1
              time.sleep(0.35)  # stay under Notion rate limit

          print(f"Notion sync complete: created={created}, already_present={len(existing_urls) - created}")
          PY
