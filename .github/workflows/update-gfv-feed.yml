name: Update feed.json (GFV mentions)

on:
  workflow_dispatch:
  schedule:
    - cron: "*/30 * * * *"

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Fetch Google News RSS and write feed.json
        run: |
          python3 - << 'PY'
          import json, re, urllib.parse, urllib.request
          import xml.etree.ElementTree as ET
          from datetime import datetime, timezone, timedelta

          MAX_ITEMS = 200
          MAX_AGE_DAYS = 180

          # Prefer UK English results (adjust if you want)
          HL = "en-GB"
          GL = "GB"
          CEID = "GB:en"

          QUERIES = [
              '"Green Flag Ventures"',
              '"Justin Zeefe"',
              '"Deborah Fairlamb"',
              'GFV -("gross floor value") -("good faith violation") -("gross floor")',
              '("Green Flag Ventures" OR "Justin Zeefe" OR "Deborah Fairlamb" OR GFV) (venture OR VC OR fund OR investment OR portfolio OR backs OR backed OR led OR seed)',
          ]

          cutoff = datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)
          cutoff_date = cutoff.date().isoformat()

          def clean_html(s: str) -> str:
              if not s:
                  return ""
              s = re.sub(r"<br\s*/?>", " ", s, flags=re.I)
              s = re.sub(r"<[^>]+>", "", s)
              s = re.sub(r"\s+", " ", s).strip()
              return s

          def parse_pubdate(s: str):
              for fmt in ("%a, %d %b %Y %H:%M:%S %Z", "%a, %d %b %Y %H:%M:%S %z"):
                  try:
                      dt = datetime.strptime(s, fmt)
                      if dt.tzinfo is None:
                          dt = dt.replace(tzinfo=timezone.utc)
                      return dt.astimezone(timezone.utc)
                  except Exception:
                      pass
              return None

          items = []
          for q in QUERIES:
              q2 = f"{q} after:{cutoff_date}"
              url = "https://news.google.com/rss/search?" + urllib.parse.urlencode({
                  "q": q2,
                  "hl": HL,
                  "gl": GL,
                  "ceid": CEID,
              })
              try:
                  with urllib.request.urlopen(url, timeout=30) as resp:
                      xml = resp.read()
              except Exception as e:
                  print("Fetch failed:", q, e)
                  continue

              root = ET.fromstring(xml)
              channel = root.find("channel")
              if channel is None:
                  continue

              for it in channel.findall("item"):
                  title = (it.findtext("title") or "").strip()
                  link = (it.findtext("link") or "").strip()
                  pub = (it.findtext("pubDate") or "").strip()
                  src_el = it.find("source")
                  source = (src_el.text or "").strip() if src_el is not None and src_el.text else ""
                  desc = clean_html((it.findtext("description") or "").strip())

                  dt = parse_pubdate(pub)
                  if not dt or dt < cutoff:
                      continue

                  # Add query string so we can store it in Notion
                  items.append({
                      "headline": title,
                      "source": source,
                      "link": link,
                      "pubDate": dt.strftime("%a, %d %b %Y %H:%M:%S GMT"),
                      "pubDateISO": dt.isoformat(),  # handy for Notion DATE
                      "snippet": desc,
                      "query": q,
                  })

          seen = set()
          out = []
          for x in sorted(items, key=lambda z: z["pubDateISO"], reverse=True):
              key = x["link"] or (x["headline"] + "|" + x["pubDate"])
              if key in seen:
                  continue
              seen.add(key)
              out.append(x)
              if len(out) >= MAX_ITEMS:
                  break

          payload = {"generatedAt": datetime.now(timezone.utc).isoformat(), "items": out}
          with open("feed.json", "w", encoding="utf-8") as f:
              json.dump(payload, f, ensure_ascii=False, indent=2)

          print("Wrote", len(out), "items")
          PY

      - name: Commit feed.json
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add feed.json
          git commit -m "Update feed.json" || exit 0
          git push

      # --- NEW: push items into Notion database ---
      - name: Install Python deps
        run: pip3 install requests

      - name: Sync feed.json to Notion database
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
        run: |
          python3 - << 'PY'
          import os, json, time
          import requests
          from datetime import datetime, timezone

          TOKEN = os.environ["NOTION_TOKEN"]
          DB_ID = os.environ["NOTION_DATABASE_ID"].replace("-", "")

          headers = {
            "Authorization": f"Bearer {TOKEN}",
            "Notion-Version": "2022-06-28",
            "Content-Type": "application/json",
          }

          with open("feed.json", "r", encoding="utf-8") as f:
            feed = json.load(f)

          items = feed.get("items", [])
          created = 0
          skipped = 0

          def query_exists(url: str) -> bool:
            payload = {
              "page_size": 1,
              "filter": {
                "property": "URL",
                "url": {"equals": url}
              }
            }
            r = requests.post(f"https://api.notion.com/v1/databases/{DB_ID}/query",
                              headers=headers, json=payload, timeout=30)
            r.raise_for_status()
            return len(r.json().get("results", [])) > 0

          for it in items:
            url = it.get("link")
            if not url:
              continue

            if query_exists(url):
              skipped += 1
              continue

            title = (it.get("headline") or "Untitled")[:180]
            source = (it.get("source") or "")[:200]
            query = (it.get("query") or "")[:200]
            iso = it.get("pubDateISO")  # already ISO8601

            props = {
              "Name": {"title": [{"text": {"content": title}}]},
              "URL": {"url": url},
              "Source": {"rich_text": [{"text": {"content": source}}]} if source else {"rich_text": []},
              "Query": {"rich_text": [{"text": {"content": query}}]} if query else {"rich_text": []},
            }

            # DATE is optional, but nice
            if isinstance(iso, str) and "T" in iso:
              props["DATE"] = {"date": {"start": iso}}

            payload = {"parent": {"database_id": DB_ID}, "properties": props}
            r = requests.post("https://api.notion.com/v1/pages",
                              headers=headers, json=payload, timeout=30)
            r.raise_for_status()
            created += 1

            # rate-limit friendly
            time.sleep(0.35)

          print(f"Notion sync done. created={created}, skipped_existing={skipped}")
          PY
